{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527196a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338f2477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('annotations/thumos_14_anno.json', 'r') as f:\n",
    "    thumos_14_anno = json.load(f)\n",
    "    \n",
    "len(thumos_14_anno['database'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THUMOSFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_dir, anno_path, max_len=100):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.max_len = max_len\n",
    "        with open(anno_path, \"r\") as f:\n",
    "            self.annotations = json.load(\n",
    "                f\n",
    "            ).database  # {\"video_name\": [{\"start\": float, \"end\": float, \"label\": int}, ...]}\n",
    "        self.video_list = list(self.annotations.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_list[idx]\n",
    "        feature_path = os.path.join(self.feature_dir, video_name + \".npy\")\n",
    "        feature = np.load(feature_path)\n",
    "\n",
    "        # Pad/Trim\n",
    "        if feature.shape[0] < self.max_len:\n",
    "            pad = np.zeros((self.max_len - feature.shape[0], feature.shape[1]))\n",
    "            feature = np.vstack([feature, pad])\n",
    "        else:\n",
    "            feature = feature[: self.max_len]\n",
    "\n",
    "        # Generate target maps\n",
    "        gt = self.annotations[video_name]  # list of actions\n",
    "        start_label = np.zeros(self.max_len)\n",
    "        end_label = np.zeros(self.max_len)\n",
    "        for a in gt:\n",
    "            start = int(a[\"segment\"][0] / 100 * self.max_len)\n",
    "            end = int(a[\"end\"][1] / 100 * self.max_len)\n",
    "            if 0 <= start < self.max_len:\n",
    "                start_label[start] = 1\n",
    "            if 0 <= end < self.max_len:\n",
    "                end_label[end] = 1\n",
    "\n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32),\n",
    "            torch.tensor(start_label, dtype=torch.float32),\n",
    "            torch.tensor(end_label, dtype=torch.float32),\n",
    "            video_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full BMN-like Temporal Action Detection Model for THUMOS14\n",
    "# Includes: Dataset loading, model, training, proposal generation, and inference.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class THUMOSFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_dir, anno_path, max_len=100):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.max_len = max_len\n",
    "        with open(anno_path, 'r') as f:\n",
    "            self.annotations = json.load(f)  # {\"video_name\": [{\"start\": float, \"end\": float, \"label\": int}, ...]}\n",
    "        self.video_list = list(self.annotations.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_list[idx]\n",
    "        feature_path = os.path.join(self.feature_dir, video_name + \".npy\")\n",
    "        feature = np.load(feature_path)\n",
    "\n",
    "        # Pad/Trim\n",
    "        if feature.shape[0] < self.max_len:\n",
    "            pad = np.zeros((self.max_len - feature.shape[0], feature.shape[1]))\n",
    "            feature = np.vstack([feature, pad])\n",
    "        else:\n",
    "            feature = feature[:self.max_len]\n",
    "\n",
    "        # Generate target maps\n",
    "        gt = self.annotations[video_name]  # list of actions\n",
    "        start_label = np.zeros(self.max_len)\n",
    "        end_label = np.zeros(self.max_len)\n",
    "        for a in gt:\n",
    "            start = int(a['start'] / 100 * self.max_len)\n",
    "            end = int(a['end'] / 100 * self.max_len)\n",
    "            if 0 <= start < self.max_len:\n",
    "                start_label[start] = 1\n",
    "            if 0 <= end < self.max_len:\n",
    "                end_label[end] = 1\n",
    "\n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32),\n",
    "            torch.tensor(start_label, dtype=torch.float32),\n",
    "            torch.tensor(end_label, dtype=torch.float32),\n",
    "            video_name\n",
    "        )\n",
    "\n",
    "# -------------------- BMN Minimal Model --------------------\n",
    "class BMNMinimal(nn.Module):\n",
    "    def __init__(self, input_dim=400, hidden_dim=256):\n",
    "        super(BMNMinimal, self).__init__()\n",
    "        self.base_conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.start_layer = nn.Conv1d(hidden_dim, 1, kernel_size=1)\n",
    "        self.end_layer = nn.Conv1d(hidden_dim, 1, kernel_size=1)\n",
    "        self.conf_layer = nn.Conv1d(hidden_dim, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D) => (B, D, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        feat = self.base_conv(x)\n",
    "        start = torch.sigmoid(self.start_layer(feat)).squeeze(1)\n",
    "        end = torch.sigmoid(self.end_layer(feat)).squeeze(1)\n",
    "        conf = torch.sigmoid(self.conf_layer(feat)).squeeze(1)\n",
    "        return start, end, conf\n",
    "\n",
    "# -------------------- Loss Function --------------------\n",
    "def bmn_loss(start_pred, end_pred, conf_pred, start_gt, end_gt):\n",
    "    start_loss = F.binary_cross_entropy(start_pred, start_gt)\n",
    "    end_loss = F.binary_cross_entropy(end_pred, end_gt)\n",
    "    conf_loss = F.mse_loss(conf_pred, torch.max(start_gt, end_gt))\n",
    "    return start_loss + end_loss + conf_loss\n",
    "\n",
    "# -------------------- Proposal Generation --------------------\n",
    "def generate_proposals(start_scores, end_scores, conf_scores, threshold=0.5):\n",
    "    proposals = []\n",
    "    T = len(start_scores)\n",
    "    for i in range(T):\n",
    "        if start_scores[i] > threshold:\n",
    "            for j in range(i + 1, T):\n",
    "                if end_scores[j] > threshold:\n",
    "                    score = (start_scores[i] * end_scores[j] * conf_scores[i])\n",
    "                    proposals.append((i, j, score.item()))\n",
    "    proposals = sorted(proposals, key=lambda x: x[2], reverse=True)\n",
    "    return proposals[:100]  # top 100\n",
    "\n",
    "# -------------------- Inference --------------------\n",
    "def run_inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for features, _, _, vid in dataloader:\n",
    "            features = features.to(device)\n",
    "            start, end, conf = model(features)\n",
    "            props = generate_proposals(start[0], end[0], conf[0])\n",
    "            results[vid[0]] = props\n",
    "    return results\n",
    "\n",
    "# -------------------- Main Script --------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    feature_dir = './data/thumos_features/'\n",
    "    anno_path = './data/thumos_val_annotations.json'\n",
    "\n",
    "    dataset = THUMOSFeatureDataset(feature_dir, anno_path)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = BMNMinimal()\n",
    "    model.to(device)\n",
    "\n",
    "    # Optional: load checkpoint\n",
    "    # model.load_state_dict(torch.load(\"bmn_model.pth\"))\n",
    "\n",
    "    results = run_inference(model, loader, device)\n",
    "    for vid, props in results.items():\n",
    "        print(f\"Video {vid}:\")\n",
    "        for s, e, score in props[:5]:\n",
    "            print(f\"  Start {s}, End {e}, Score {score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
