{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "0c82a253", "cell_type": "markdown", "source": "# THUMOS14 Action Recognition with Temporal Localization using PyTorch", "metadata": {}}, {"id": "77a87d49", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import os\nimport cv2\nimport glob\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom moviepy.editor import VideoFileClip\nfrom collections import defaultdict\n", "outputs": []}, {"id": "228db25d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Set paths\nVIDEO_DIR = \"/mnt/data/thumos14_action_recognition/videos\"\nANNOTATION_DIR = \"/mnt/data/thumos14_action_recognition/annotations\"\nFRAME_DIR = \"/mnt/data/thumos14_action_recognition/frames\"\n\nos.makedirs(VIDEO_DIR, exist_ok=True)\nos.makedirs(ANNOTATION_DIR, exist_ok=True)\nos.makedirs(FRAME_DIR, exist_ok=True)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n", "outputs": []}, {"id": "d4722256", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def parse_annotations(annotation_dir):\n    annotations = defaultdict(list)\n    class_map = {}\n    for idx, file in enumerate(os.listdir(annotation_dir)):\n        class_name = file.replace('.txt', '')\n        class_map[class_name] = idx\n        with open(os.path.join(annotation_dir, file), 'r') as f:\n            for line in f:\n                video_id, start, end = line.strip().split()\n                annotations[video_id].append({\n                    'start': float(start),\n                    'end': float(end),\n                    'label': class_name\n                })\n    return annotations, class_map\n\nannotations, class_map = parse_annotations(ANNOTATION_DIR)\nprint(\"Classes:\", class_map)\n", "outputs": []}, {"id": "9172d6e5", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def extract_frames_from_video(video_path, output_folder, fps=10):\n    os.makedirs(output_folder, exist_ok=True)\n    clip = VideoFileClip(video_path)\n    duration = clip.duration\n    for t in np.arange(0, duration, 1.0 / fps):\n        frame = clip.get_frame(t)\n        frame_path = os.path.join(output_folder, f\"{int(t * fps):05d}.jpg\")\n        cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n    clip.close()\n\n# Only run this once\n# for video_file in os.listdir(VIDEO_DIR):\n#     video_id = video_file.replace('.mp4', '')\n#     extract_frames_from_video(os.path.join(VIDEO_DIR, video_file), os.path.join(FRAME_DIR, video_id))\n", "outputs": []}, {"id": "9b6201ff", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "class THUMOS14Dataset(Dataset):\n    def __init__(self, annotations, class_map, frame_root, transform=None, clip_len=16, fps=10):\n        self.samples = []\n        self.class_map = class_map\n        self.transform = transform\n        self.clip_len = clip_len\n        self.fps = fps\n        for video_id, anns in annotations.items():\n            frame_dir = os.path.join(frame_root, video_id)\n            if not os.path.exists(frame_dir): continue\n            for ann in anns:\n                start_f = int(float(ann['start']) * fps)\n                end_f = int(float(ann['end']) * fps)\n                for i in range(start_f, end_f - clip_len + 1, clip_len):\n                    self.samples.append({\n                        'video_id': video_id,\n                        'start': i,\n                        'label': class_map[ann['label']]\n                    })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        video_id = sample['video_id']\n        start = sample['start']\n        label = sample['label']\n\n        frames = []\n        for i in range(start, start + self.clip_len):\n            frame_path = os.path.join(FRAME_DIR, video_id, f\"{i:05d}.jpg\")\n            img = cv2.imread(frame_path)\n            if img is None:\n                img = np.zeros((240, 320, 3), dtype=np.uint8)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if self.transform:\n                img = self.transform(img)\n            else:\n                img = T.ToTensor()(img)\n            frames.append(img)\n\n        return torch.stack(frames), torch.tensor(label)\n", "outputs": []}, {"id": "307df9ad", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "class SimpleConvGRU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n        self.rnn = nn.GRU(32 * 60 * 80, 128, batch_first=True)\n        self.fc = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        b, t, c, h, w = x.shape\n        x = x.view(b * t, c, h, w)\n        x = self.conv(x)\n        x = x.view(b, t, -1)\n        _, h_n = self.rnn(x)\n        return self.fc(h_n[-1])\n", "outputs": []}, {"id": "7e59fd0d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def train(model, loader, criterion, optimizer, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for X, y in loader:\n            X, y = X.to(DEVICE), y.to(DEVICE)\n            out = model(X)\n            loss = criterion(out, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\")\n", "outputs": []}, {"id": "fb995664", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "transform = T.Compose([\n    T.ToPILImage(),\n    T.Resize((240, 320)),\n    T.ToTensor()\n])\n\ndataset = THUMOS14Dataset(annotations, class_map, FRAME_DIR, transform=transform)\nloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\nmodel = SimpleConvGRU(num_classes=len(class_map)).to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# train(model, loader, criterion, optimizer)\n", "outputs": []}, {"id": "fb7ef8fb", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def predict(model, video_clip):\n    model.eval()\n    with torch.no_grad():\n        video_clip = video_clip.unsqueeze(0).to(DEVICE)\n        logits = model(video_clip)\n        pred = torch.argmax(logits, dim=1)\n    return pred.item()\n\n# Example:\n# X, y = dataset[0]\n# pred = predict(model, X)\n# print(\"GT:\", y.item(), \"Pred:\", pred)\n", "outputs": []}]}